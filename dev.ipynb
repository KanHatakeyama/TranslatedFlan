{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/setup/miniconda3/envs/llmeval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-24 17:27:44,737\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from vllm import SamplingParams, LLM\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from src.generator import prepare_records\n",
    "from src.clean_utils import clean\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wait_time = random.randint(1, 10)\n",
    "time.sleep(wait_time)\n",
    "\n",
    "#####################\n",
    "# 設定関連\n",
    "n_records = 30\n",
    "out_dir=\"0624out_data_flan\"\n",
    "\n",
    "#parquet一覧\n",
    "directory_path = '/data/2024/TranslatedFlan/FLAN/'\n",
    "#tubame\n",
    "####################\n",
    "\n",
    "################\n",
    "#メイン\n",
    "\n",
    "os.system(f\"mkdir -p {out_dir}\")\n",
    "\n",
    "current_time_no_symbols = datetime.now().strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\").replace(\"-\", \"\").replace(\":\", \"\").replace(\" \", \"\")\n",
    "out_path = f\"{out_dir}/model_{current_time_no_symbols}.jsonl\"\n",
    "\n",
    "\n",
    "# 再帰的にディレクトリ内のすべてのパーケットファイルを検索する\n",
    "#parquet_list = glob.glob(f\"{directory_path}/*/*.*.parquet\", recursive=False)\n",
    "\n",
    "# 再帰的にディレクトリ内のすべてのパーケットファイルを検索する\n",
    "parquet_list = glob.glob(os.path.join(directory_path, '**', '*.parquet'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'random' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     parquet_path\u001b[38;5;241m=\u001b[39m\u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mchoice(parquet_list)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#ファイルサイズを確認\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[0;32m---> 14\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshuffling\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    try:\n",
    "        parquet_path=random.choice(parquet_list)\n",
    "        #ファイルサイズを確認\n",
    "        file_size = os.path.getsize(parquet_path)\n",
    "        if file_size <1000:\n",
    "            continue\n",
    "        print(f\"load {parquet_path}\")\n",
    "        df=pd.read_parquet(parquet_path)\n",
    "        ds= Dataset.from_pandas(df)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(10)\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "print(\"shuffling\")\n",
    "ds = ds.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init llm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/setup/miniconda3/envs/llmeval/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-24 17:25:16 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='microsoft/Phi-3-medium-128k-instruct', speculative_config=None, tokenizer='microsoft/Phi-3-medium-128k-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=20000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=microsoft/Phi-3-medium-128k-instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-24 17:25:17 utils.py:660] Found nccl from library /home/setup/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 06-24 17:25:18 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 06-24 17:25:19 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 06-24 17:25:23 model_runner.py:175] Loading model weights took 26.1473 GB\n",
      "INFO 06-24 17:25:26 gpu_executor.py:114] # GPU blocks: 13470, # CPU blocks: 1310\n",
      "INFO 06-24 17:25:27 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-24 17:25:27 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-24 17:25:32 model_runner.py:1017] Graph capturing finished in 5 secs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"init llm\")\n",
    "model_name = \"microsoft/Phi-3-medium-128k-instruct\"\n",
    "llm = LLM(model=model_name, trust_remote_code=True,\n",
    "          max_model_len=20000\n",
    "          )\n",
    "inst_dict = {   \n",
    "\n",
    "\"translate1\":\"\"\"You are a professional translator. Translate the following Englsh into fluent Japanese.\n",
    "#English\"\"\",\n",
    "\"translate2\":\"\"\"You are a translator. Translate the following Englsh into fluent Japanese.\n",
    "#English\"\"\",\n",
    "\"translate3\":\"\"\"You are a professional translator. Translate the following Englsh into fluent Japanese.\n",
    "Use formal Japanese.\n",
    "#English\"\"\",\n",
    "\"translate4\":\"\"\"You are a professional translator. Translate the following Englsh into fluent Japanese.\n",
    "Use polite Japanese.\n",
    "#English\"\"\",\n",
    "\"translate5\":\"\"\"You are a professional translator. Translate the following Englsh into fluent Japanese.\n",
    "Use casual Japanese.\n",
    "#English\"\"\",\n",
    "\n",
    "}              \n",
    "\n",
    "       \n",
    "\n",
    "mode_list = list(inst_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85978  records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 31/31 [01:09<00:00,  2.23s/it]\n",
      "Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "print(len(ds), \" records\")\n",
    "while True:\n",
    "\n",
    "    #回答\n",
    "    records = prepare_records(\n",
    "        ds, mode_list, n_records=n_records,\n",
    "        inst_dict=inst_dict\n",
    "        \n",
    "        )\n",
    "    prompts = [record[\"original_text\"] for record in records]\n",
    "    outputs1 = llm.generate(\n",
    "        prompts,\n",
    "        sampling_params=SamplingParams(\n",
    "            temperature=0.1,\n",
    "            max_tokens=2048,\n",
    "            repetition_penalty=1.15,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for record, ja_output in zip(records, outputs1):\n",
    "        ja= (ja_output.outputs[0].text).strip()\n",
    "\n",
    "        ja=clean(ja,lang=\"ja\")\n",
    "\n",
    "        if ja==\"\":\n",
    "            #print(\"rejected\")\n",
    "            #print(ja_output.outputs[0].text)\n",
    "            continue\n",
    "\n",
    "\n",
    "        record[\"ja\"]=ja \n",
    "        #record.pop(\"original_text\")\n",
    "\n",
    "\n",
    "        #print(\"saving to \"+out_path)\n",
    "        with open(out_path, \"a\") as f:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
