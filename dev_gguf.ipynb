{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/setup/miniconda3/envs/llmeval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-25 14:26:06,072\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from vllm import SamplingParams, LLM\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from src.generator import prepare_records\n",
    "from src.clean_utils import clean\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wait_time = random.randint(1, 10)\n",
    "time.sleep(wait_time)\n",
    "\n",
    "#####################\n",
    "# 設定関連\n",
    "n_records = 2\n",
    "out_dir=\"0625out_data_flan_mixtral\"\n",
    "\n",
    "#parquet一覧\n",
    "directory_path = '/data/2024/FLAN/'\n",
    "#tubame\n",
    "####################\n",
    "\n",
    "################\n",
    "#メイン\n",
    "\n",
    "os.system(f\"mkdir -p {out_dir}\")\n",
    "\n",
    "current_time_no_symbols = datetime.now().strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\").replace(\"-\", \"\").replace(\":\", \"\").replace(\" \", \"\")\n",
    "out_path = f\"{out_dir}/model_{current_time_no_symbols}.jsonl\"\n",
    "\n",
    "\n",
    "# 再帰的にディレクトリ内のすべてのパーケットファイルを検索する\n",
    "#parquet_list = glob.glob(f\"{directory_path}/*/*.*.parquet\", recursive=False)\n",
    "\n",
    "# 再帰的にディレクトリ内のすべてのパーケットファイルを検索する\n",
    "parquet_list = glob.glob(os.path.join(directory_path, '**', '*.parquet'), recursive=True)\n",
    "\n",
    "parquet_list=[i for i in parquet_list if i.find(\"cot\")>0]\n",
    "print(len(parquet_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load /data/2024/FLAN/cot_zsopt_data/part.0.parquet\n",
      "shuffling\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    try:\n",
    "        parquet_path=random.choice(parquet_list)\n",
    "        #ファイルサイズを確認\n",
    "        file_size = os.path.getsize(parquet_path)\n",
    "        if file_size <1000:\n",
    "            continue\n",
    "        print(f\"load {parquet_path}\")\n",
    "        df=pd.read_parquet(parquet_path)\n",
    "        ds= Dataset.from_pandas(df)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        #time.sleep(10)\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "print(\"shuffling\")\n",
    "ds = ds.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: additional 3 GGUFs metadata loaded.\n",
      "llama_model_loader: loaded meta data with 29 key-value pairs and 563 tensors from /data/2023/1505llmmatsu/mixtral_gguf/model/Mixtral-8x22B-Instruct-v0.1.Q5_K_M-00001-of-00004.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models--mistralai--Mixtral-8x22B-Inst...\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 56\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 65536\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 6144\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 48\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  11:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  13:                           llama.vocab_size u32              = 32768\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32768]   = [-1000.000000, -1000.000000, -1000.00...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32768]   = [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{bos_token}}{% for message in messag...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  26:                                   split.no u16              = 0\n",
      "llama_model_loader: - kv  27:                                split.count u16              = 4\n",
      "llama_model_loader: - kv  28:                        split.tensors.count i32              = 563\n",
      "llama_model_loader: - type  f32:  113 tensors\n",
      "llama_model_loader: - type  f16:   56 tensors\n",
      "llama_model_loader: - type q8_0:  112 tensors\n",
      "llama_model_loader: - type q5_K:  253 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 1027/32768 vs 259/32768 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32768\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 65536\n",
      "llm_load_print_meta: n_embd           = 6144\n",
      "llm_load_print_meta: n_head           = 48\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 56\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 6\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 65536\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8x22B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 140.63 B\n",
      "llm_load_print_meta: model size       = 93.11 GiB (5.69 BPW) \n",
      "llm_load_print_meta: general.name     = models--mistralai--Mixtral-8x22B-Instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 781 '<0x0A>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: no\n",
      "ggml_cuda_init: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes\n",
      "  Device 1: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.84 MiB\n",
      "llm_load_tensors: offloading 56 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 57/57 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   132.00 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 49173.33 MiB\n",
      "llm_load_tensors:      CUDA1 buffer size = 46038.07 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   464.00 MiB\n",
      "llama_kv_cache_init:      CUDA1 KV buffer size =   432.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   512.01 MiB\n",
      "llama_new_context_with_model:      CUDA1 compute buffer size =   512.02 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    44.02 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2638\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'split.no': '0', 'tokenizer.chat_template': \"{{bos_token}}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ ' [INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + ' ' + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'split.count': '4', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '32768', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '6144', 'llama.feed_forward_length': '16384', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '48', 'llama.block_count': '56', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'split.tensors.count': '563', 'llama.context_length': '65536', 'general.name': 'models--mistralai--Mixtral-8x22B-Instruct-v0.1', 'llama.expert_used_count': '2', 'general.file_type': '17'}\n",
      "Using gguf chat template: {{bos_token}}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ ' [INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + ' ' + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "model_path=\"/data/2023/1505llmmatsu/mixtral_gguf/model/Mixtral-8x22B-Instruct-v0.1.Q5_K_M-00001-of-00004.gguf\"\n",
    "\n",
    "from llama_cpp import Llama\n",
    "class GGUFBot:\n",
    "    def __init__(self, model_path=\"model/Mixtral-8x22B-Instruct-v0.1.Q5_K_M-00001-of-00004.gguf\",\n",
    "                 max_new_tokens=4000,\n",
    "                 n_gpu_layers=100,\n",
    "                 n_ctx=4096) -> None:\n",
    "        print(\"loading model...\")\n",
    "\n",
    "        self.model = Llama(model_path=model_path,\n",
    "                           n_ctx=n_ctx, n_gpu_layers=n_gpu_layers, )\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def ask(self, question):\n",
    "\n",
    "        prompt = f\"\"\"<s>[INST]{question}[/INST]#日本語訳\\n\"\"\"\n",
    "\n",
    "        output = self.model(\n",
    "            prompt,\n",
    "            max_tokens=self.max_new_tokens,\n",
    "            # temperature = 0.7,\n",
    "            # top_p = 0.8,\n",
    "            # repeat_penalty = 1.1,\n",
    "            # frequency_penalty = 1.0,\n",
    "            # presence_penalty = 1.0,\n",
    "            # stop = [\"\\n###  Instruction:\", \"\\n### Response:\", \"\\n\"],\n",
    "            # echo = True,\n",
    "        )\n",
    "        return output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "\n",
    "bot=GGUFBot(model_path=model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init llm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"init llm\")\n",
    "#model_name = \"microsoft/Phi-3-medium-128k-instruct\"\n",
    "#llm = LLM(model=model_name, trust_remote_code=True,\n",
    "#          max_model_len=20000\n",
    "#          )\n",
    "inst_dict = {   \n",
    "\n",
    "\"translate1\":\"\"\"You are a professional translator. Translate the following Englsh into fluent Japanese.\n",
    "Output only the translated Japanese sentence.\n",
    "#English\"\"\",\n",
    "\n",
    "\"translate2\":\"\"\"You are a translator. Translate the following Englsh into fluent Japanese.\n",
    "Output only the translated Japanese sentence.\n",
    "#English\"\"\",\n",
    "\n",
    "\"translate3\":\"\"\"You are a professional translator. Translate the following Englsh into fluent Japanese.\n",
    "Use formal Japanese.\n",
    "Output only the translated Japanese sentence.\n",
    "#English\"\"\",\n",
    "\n",
    "\"translate4\":\"\"\"You are a professional translator. Translate the following Englsh into fluent Japanese.\n",
    "Use polite Japanese.\n",
    "Output only the translated Japanese sentence.\n",
    "#English\"\"\",\n",
    "\n",
    "\"translate5\":\"\"\"You are a professional translator. Translate the following Englsh into fluent Japanese.\n",
    "Use casual Japanese.\n",
    "Output only the translated Japanese sentence.\n",
    "#English\"\"\",\n",
    "\n",
    "}              \n",
    "\n",
    "       \n",
    "\n",
    "mode_list = list(inst_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bot.ask(\"元気ですか?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_records(ds, mode_list,\n",
    "                    inst_dict,\n",
    "                    n_records=300,\n",
    "                    ):\n",
    "    ds = ds.shuffle()\n",
    "\n",
    "    records = []\n",
    "    cnt = 0\n",
    "    for record in ds:\n",
    "        mode = random.choice(mode_list)\n",
    "        inst = inst_dict[mode]\n",
    "\n",
    "        text = record[\"inputs\"]+\"\\n\"+record[\"targets\"]\n",
    "        prompt= f\"\"\"{inst}{text}\"\"\"\n",
    "\n",
    "        records.append(\n",
    "            {\"prompt\": prompt,\n",
    "                \"mode\": mode,\n",
    "                \"en\":text\n",
    "             }\n",
    "        )\n",
    "        cnt += 1\n",
    "        if cnt > n_records:\n",
    "            break\n",
    "\n",
    "    return records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95570  records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      50.26 ms /   321 runs   (    0.16 ms per token,  6386.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1498.17 ms /   208 tokens (    7.20 ms per token,   138.84 tokens per second)\n",
      "llama_print_timings:        eval time =   10880.35 ms /   320 runs   (   34.00 ms per token,    29.41 tokens per second)\n",
      "llama_print_timings:       total time =   12794.27 ms /   528 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      60.83 ms /   374 runs   (    0.16 ms per token,  6148.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     991.22 ms /   162 tokens (    6.12 ms per token,   163.44 tokens per second)\n",
      "llama_print_timings:        eval time =   12655.86 ms /   373 runs   (   33.93 ms per token,    29.47 tokens per second)\n",
      "llama_print_timings:       total time =   14120.12 ms /   535 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      15.80 ms /    97 runs   (    0.16 ms per token,  6138.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     940.39 ms /   120 tokens (    7.84 ms per token,   127.61 tokens per second)\n",
      "llama_print_timings:        eval time =    3246.33 ms /    96 runs   (   33.82 ms per token,    29.57 tokens per second)\n",
      "llama_print_timings:       total time =    4301.59 ms /   216 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      69.10 ms /   416 runs   (    0.17 ms per token,  6020.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     933.93 ms /   100 tokens (    9.34 ms per token,   107.07 tokens per second)\n",
      "llama_print_timings:        eval time =   14088.05 ms /   415 runs   (   33.95 ms per token,    29.46 tokens per second)\n",
      "llama_print_timings:       total time =   15562.10 ms /   515 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      55.04 ms /   353 runs   (    0.16 ms per token,  6413.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1177.56 ms /   240 tokens (    4.91 ms per token,   203.81 tokens per second)\n",
      "llama_print_timings:        eval time =   11969.36 ms /   352 runs   (   34.00 ms per token,    29.41 tokens per second)\n",
      "llama_print_timings:       total time =   13596.02 ms /   592 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      23.34 ms /   146 runs   (    0.16 ms per token,  6254.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     933.91 ms /   112 tokens (    8.34 ms per token,   119.93 tokens per second)\n",
      "llama_print_timings:        eval time =    4906.14 ms /   145 runs   (   33.84 ms per token,    29.55 tokens per second)\n",
      "llama_print_timings:       total time =    6017.62 ms /   257 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      30.82 ms /   191 runs   (    0.16 ms per token,  6196.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1568.55 ms /   311 tokens (    5.04 ms per token,   198.27 tokens per second)\n",
      "llama_print_timings:        eval time =    6451.44 ms /   190 runs   (   33.95 ms per token,    29.45 tokens per second)\n",
      "llama_print_timings:       total time =    8257.16 ms /   501 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      22.27 ms /   137 runs   (    0.16 ms per token,  6151.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     938.26 ms /   112 tokens (    8.38 ms per token,   119.37 tokens per second)\n",
      "llama_print_timings:        eval time =    4602.71 ms /   136 runs   (   33.84 ms per token,    29.55 tokens per second)\n",
      "llama_print_timings:       total time =    5708.35 ms /   248 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      27.80 ms /   181 runs   (    0.15 ms per token,  6511.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     912.09 ms /    84 tokens (   10.86 ms per token,    92.10 tokens per second)\n",
      "llama_print_timings:        eval time =    6083.89 ms /   180 runs   (   33.80 ms per token,    29.59 tokens per second)\n",
      "llama_print_timings:       total time =    7215.16 ms /   264 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      10.47 ms /    63 runs   (    0.17 ms per token,  6016.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     933.96 ms /    94 tokens (    9.94 ms per token,   100.65 tokens per second)\n",
      "llama_print_timings:        eval time =    2097.58 ms /    62 runs   (   33.83 ms per token,    29.56 tokens per second)\n",
      "llama_print_timings:       total time =    3106.86 ms /   156 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      15.16 ms /    89 runs   (    0.17 ms per token,  5870.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     857.46 ms /    61 tokens (   14.06 ms per token,    71.14 tokens per second)\n",
      "llama_print_timings:        eval time =    2974.19 ms /    88 runs   (   33.80 ms per token,    29.59 tokens per second)\n",
      "llama_print_timings:       total time =    3939.85 ms /   149 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      18.65 ms /   115 runs   (    0.16 ms per token,  6166.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1053.93 ms /   187 tokens (    5.64 ms per token,   177.43 tokens per second)\n",
      "llama_print_timings:        eval time =    3862.03 ms /   114 runs   (   33.88 ms per token,    29.52 tokens per second)\n",
      "llama_print_timings:       total time =    5056.02 ms /   301 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      19.32 ms /   120 runs   (    0.16 ms per token,  6209.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     931.37 ms /   108 tokens (    8.62 ms per token,   115.96 tokens per second)\n",
      "llama_print_timings:        eval time =    4021.33 ms /   119 runs   (   33.79 ms per token,    29.59 tokens per second)\n",
      "llama_print_timings:       total time =    5096.62 ms /   227 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      22.23 ms /   138 runs   (    0.16 ms per token,  6206.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     921.89 ms /    99 tokens (    9.31 ms per token,   107.39 tokens per second)\n",
      "llama_print_timings:        eval time =    4631.34 ms /   137 runs   (   33.81 ms per token,    29.58 tokens per second)\n",
      "llama_print_timings:       total time =    5718.60 ms /   236 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      20.19 ms /   126 runs   (    0.16 ms per token,  6241.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     971.31 ms /   132 tokens (    7.36 ms per token,   135.90 tokens per second)\n",
      "llama_print_timings:        eval time =    4222.96 ms /   125 runs   (   33.78 ms per token,    29.60 tokens per second)\n",
      "llama_print_timings:       total time =    5344.82 ms /   257 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      21.27 ms /   127 runs   (    0.17 ms per token,  5970.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     934.65 ms /   111 tokens (    8.42 ms per token,   118.76 tokens per second)\n",
      "llama_print_timings:        eval time =    4258.83 ms /   126 runs   (   33.80 ms per token,    29.59 tokens per second)\n",
      "llama_print_timings:       total time =    5347.19 ms /   237 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      16.35 ms /   100 runs   (    0.16 ms per token,  6116.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     943.32 ms /   118 tokens (    7.99 ms per token,   125.09 tokens per second)\n",
      "llama_print_timings:        eval time =    3346.20 ms /    99 runs   (   33.80 ms per token,    29.59 tokens per second)\n",
      "llama_print_timings:       total time =    4410.91 ms /   217 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      30.21 ms /   192 runs   (    0.16 ms per token,  6355.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     996.37 ms /   165 tokens (    6.04 ms per token,   165.60 tokens per second)\n",
      "llama_print_timings:        eval time =    6466.06 ms /   191 runs   (   33.85 ms per token,    29.54 tokens per second)\n",
      "llama_print_timings:       total time =    7698.82 ms /   356 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      56.01 ms /   351 runs   (    0.16 ms per token,  6267.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     972.85 ms /   143 tokens (    6.80 ms per token,   146.99 tokens per second)\n",
      "llama_print_timings:        eval time =   11867.20 ms /   350 runs   (   33.91 ms per token,    29.49 tokens per second)\n",
      "llama_print_timings:       total time =   13288.38 ms /   493 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      32.77 ms /   206 runs   (    0.16 ms per token,  6287.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     996.64 ms /   164 tokens (    6.08 ms per token,   164.55 tokens per second)\n",
      "llama_print_timings:        eval time =    6945.35 ms /   205 runs   (   33.88 ms per token,    29.52 tokens per second)\n",
      "llama_print_timings:       total time =    8198.98 ms /   369 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      53.99 ms /   331 runs   (    0.16 ms per token,  6131.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     977.29 ms /   138 tokens (    7.08 ms per token,   141.21 tokens per second)\n",
      "llama_print_timings:        eval time =   11178.96 ms /   330 runs   (   33.88 ms per token,    29.52 tokens per second)\n",
      "llama_print_timings:       total time =   12578.19 ms /   468 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      30.31 ms /   180 runs   (    0.17 ms per token,  5938.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     970.10 ms /   134 tokens (    7.24 ms per token,   138.13 tokens per second)\n",
      "llama_print_timings:        eval time =    6063.93 ms /   179 runs   (   33.88 ms per token,    29.52 tokens per second)\n",
      "llama_print_timings:       total time =    7260.56 ms /   313 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      27.62 ms /   176 runs   (    0.16 ms per token,  6372.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     940.98 ms /   119 tokens (    7.91 ms per token,   126.46 tokens per second)\n",
      "llama_print_timings:        eval time =    5922.88 ms /   175 runs   (   33.85 ms per token,    29.55 tokens per second)\n",
      "llama_print_timings:       total time =    7081.20 ms /   294 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      23.72 ms /   149 runs   (    0.16 ms per token,  6281.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     919.69 ms /    93 tokens (    9.89 ms per token,   101.12 tokens per second)\n",
      "llama_print_timings:        eval time =    5007.03 ms /   148 runs   (   33.83 ms per token,    29.56 tokens per second)\n",
      "llama_print_timings:       total time =    6108.46 ms /   241 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      28.05 ms /   175 runs   (    0.16 ms per token,  6238.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     975.18 ms /   143 tokens (    6.82 ms per token,   146.64 tokens per second)\n",
      "llama_print_timings:        eval time =    5890.46 ms /   174 runs   (   33.85 ms per token,    29.54 tokens per second)\n",
      "llama_print_timings:       total time =    7081.65 ms /   317 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      29.21 ms /   180 runs   (    0.16 ms per token,  6161.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     925.14 ms /    99 tokens (    9.34 ms per token,   107.01 tokens per second)\n",
      "llama_print_timings:        eval time =    6059.02 ms /   179 runs   (   33.85 ms per token,    29.54 tokens per second)\n",
      "llama_print_timings:       total time =    7207.62 ms /   278 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =     109.70 ms /   683 runs   (    0.16 ms per token,  6226.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1504.39 ms /   368 tokens (    4.09 ms per token,   244.62 tokens per second)\n",
      "llama_print_timings:        eval time =   23355.54 ms /   682 runs   (   34.25 ms per token,    29.20 tokens per second)\n",
      "llama_print_timings:       total time =   25808.32 ms /  1050 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      37.60 ms /   246 runs   (    0.15 ms per token,  6542.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1037.55 ms /   182 tokens (    5.70 ms per token,   175.41 tokens per second)\n",
      "llama_print_timings:        eval time =    8302.02 ms /   245 runs   (   33.89 ms per token,    29.51 tokens per second)\n",
      "llama_print_timings:       total time =    9644.42 ms /   427 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =       5.34 ms /    32 runs   (    0.17 ms per token,  5992.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     935.14 ms /   104 tokens (    8.99 ms per token,   111.21 tokens per second)\n",
      "llama_print_timings:        eval time =    1047.93 ms /    31 runs   (   33.80 ms per token,    29.58 tokens per second)\n",
      "llama_print_timings:       total time =    2020.56 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      31.16 ms /   191 runs   (    0.16 ms per token,  6129.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     968.90 ms /   131 tokens (    7.40 ms per token,   135.20 tokens per second)\n",
      "llama_print_timings:        eval time =    6430.50 ms /   190 runs   (   33.84 ms per token,    29.55 tokens per second)\n",
      "llama_print_timings:       total time =    7636.00 ms /   321 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      77.20 ms /   475 runs   (    0.16 ms per token,  6152.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1310.79 ms /   290 tokens (    4.52 ms per token,   221.24 tokens per second)\n",
      "llama_print_timings:        eval time =   16154.71 ms /   474 runs   (   34.08 ms per token,    29.34 tokens per second)\n",
      "llama_print_timings:       total time =   18094.40 ms /   764 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      34.34 ms /   215 runs   (    0.16 ms per token,  6261.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     973.02 ms /   132 tokens (    7.37 ms per token,   135.66 tokens per second)\n",
      "llama_print_timings:        eval time =    7249.78 ms /   214 runs   (   33.88 ms per token,    29.52 tokens per second)\n",
      "llama_print_timings:       total time =    8491.80 ms /   346 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      20.83 ms /   131 runs   (    0.16 ms per token,  6288.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     906.61 ms /    76 tokens (   11.93 ms per token,    83.83 tokens per second)\n",
      "llama_print_timings:        eval time =    4394.84 ms /   130 runs   (   33.81 ms per token,    29.58 tokens per second)\n",
      "llama_print_timings:       total time =    5462.40 ms /   206 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      48.93 ms /   299 runs   (    0.16 ms per token,  6110.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1058.88 ms /   193 tokens (    5.49 ms per token,   182.27 tokens per second)\n",
      "llama_print_timings:        eval time =   10106.44 ms /   298 runs   (   33.91 ms per token,    29.49 tokens per second)\n",
      "llama_print_timings:       total time =   11547.67 ms /   491 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      35.03 ms /   212 runs   (    0.17 ms per token,  6051.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     969.61 ms /   130 tokens (    7.46 ms per token,   134.08 tokens per second)\n",
      "llama_print_timings:        eval time =    7141.77 ms /   211 runs   (   33.85 ms per token,    29.54 tokens per second)\n",
      "llama_print_timings:       total time =    8377.26 ms /   341 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      23.84 ms /   142 runs   (    0.17 ms per token,  5957.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     938.04 ms /   107 tokens (    8.77 ms per token,   114.07 tokens per second)\n",
      "llama_print_timings:        eval time =    4770.58 ms /   141 runs   (   33.83 ms per token,    29.56 tokens per second)\n",
      "llama_print_timings:       total time =    5884.26 ms /   248 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      41.22 ms /   245 runs   (    0.17 ms per token,  5943.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1085.96 ms /   206 tokens (    5.27 ms per token,   189.69 tokens per second)\n",
      "llama_print_timings:        eval time =    8270.60 ms /   244 runs   (   33.90 ms per token,    29.50 tokens per second)\n",
      "llama_print_timings:       total time =    9667.23 ms /   450 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      12.22 ms /    73 runs   (    0.17 ms per token,  5975.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     939.96 ms /   107 tokens (    8.78 ms per token,   113.83 tokens per second)\n",
      "llama_print_timings:        eval time =    2435.06 ms /    72 runs   (   33.82 ms per token,    29.57 tokens per second)\n",
      "llama_print_timings:       total time =    3462.84 ms /   179 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      13.59 ms /    82 runs   (    0.17 ms per token,  6034.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     885.54 ms /    72 tokens (   12.30 ms per token,    81.31 tokens per second)\n",
      "llama_print_timings:        eval time =    2734.88 ms /    81 runs   (   33.76 ms per token,    29.62 tokens per second)\n",
      "llama_print_timings:       total time =    3718.75 ms /   153 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      22.45 ms /   138 runs   (    0.16 ms per token,  6146.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     923.98 ms /    86 tokens (   10.74 ms per token,    93.08 tokens per second)\n",
      "llama_print_timings:        eval time =    4631.32 ms /   137 runs   (   33.81 ms per token,    29.58 tokens per second)\n",
      "llama_print_timings:       total time =    5722.52 ms /   223 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      37.49 ms /   227 runs   (    0.17 ms per token,  6054.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     948.83 ms /   125 tokens (    7.59 ms per token,   131.74 tokens per second)\n",
      "llama_print_timings:        eval time =    7647.89 ms /   226 runs   (   33.84 ms per token,    29.55 tokens per second)\n",
      "llama_print_timings:       total time =    8880.33 ms /   351 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      24.38 ms /   146 runs   (    0.17 ms per token,  5989.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1086.27 ms /   202 tokens (    5.38 ms per token,   185.96 tokens per second)\n",
      "llama_print_timings:        eval time =    4909.45 ms /   145 runs   (   33.86 ms per token,    29.53 tokens per second)\n",
      "llama_print_timings:       total time =    6176.34 ms /   347 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      77.48 ms /   502 runs   (    0.15 ms per token,  6478.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1468.82 ms /   352 tokens (    4.17 ms per token,   239.65 tokens per second)\n",
      "llama_print_timings:        eval time =   17102.10 ms /   501 runs   (   34.14 ms per token,    29.29 tokens per second)\n",
      "llama_print_timings:       total time =   19229.71 ms /   853 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      28.44 ms /   175 runs   (    0.16 ms per token,  6152.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     974.70 ms /   134 tokens (    7.27 ms per token,   137.48 tokens per second)\n",
      "llama_print_timings:        eval time =    5889.79 ms /   174 runs   (   33.85 ms per token,    29.54 tokens per second)\n",
      "llama_print_timings:       total time =    7082.18 ms /   308 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      36.27 ms /   225 runs   (    0.16 ms per token,  6203.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     975.21 ms /   135 tokens (    7.22 ms per token,   138.43 tokens per second)\n",
      "llama_print_timings:        eval time =    7579.97 ms /   224 runs   (   33.84 ms per token,    29.55 tokens per second)\n",
      "llama_print_timings:       total time =    8833.68 ms /   359 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      31.59 ms /   199 runs   (    0.16 ms per token,  6300.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     950.15 ms /   126 tokens (    7.54 ms per token,   132.61 tokens per second)\n",
      "llama_print_timings:        eval time =    6700.48 ms /   198 runs   (   33.84 ms per token,    29.55 tokens per second)\n",
      "llama_print_timings:       total time =    7895.34 ms /   324 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      25.99 ms /   156 runs   (    0.17 ms per token,  6001.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     929.06 ms /   104 tokens (    8.93 ms per token,   111.94 tokens per second)\n",
      "llama_print_timings:        eval time =    5236.53 ms /   155 runs   (   33.78 ms per token,    29.60 tokens per second)\n",
      "llama_print_timings:       total time =    6356.81 ms /   259 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      20.55 ms /   126 runs   (    0.16 ms per token,  6132.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     968.73 ms /   129 tokens (    7.51 ms per token,   133.16 tokens per second)\n",
      "llama_print_timings:        eval time =    4225.81 ms /   125 runs   (   33.81 ms per token,    29.58 tokens per second)\n",
      "llama_print_timings:       total time =    5347.62 ms /   254 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      33.50 ms /   209 runs   (    0.16 ms per token,  6238.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     934.27 ms /   104 tokens (    8.98 ms per token,   111.32 tokens per second)\n",
      "llama_print_timings:        eval time =    7033.07 ms /   208 runs   (   33.81 ms per token,    29.57 tokens per second)\n",
      "llama_print_timings:       total time =    8223.92 ms /   312 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      29.50 ms /   181 runs   (    0.16 ms per token,  6135.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     945.11 ms /   123 tokens (    7.68 ms per token,   130.14 tokens per second)\n",
      "llama_print_timings:        eval time =    6090.38 ms /   180 runs   (   33.84 ms per token,    29.55 tokens per second)\n",
      "llama_print_timings:       total time =    7259.53 ms /   303 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      88.49 ms /   556 runs   (    0.16 ms per token,  6283.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     967.04 ms /   132 tokens (    7.33 ms per token,   136.50 tokens per second)\n",
      "llama_print_timings:        eval time =   18861.07 ms /   555 runs   (   33.98 ms per token,    29.43 tokens per second)\n",
      "llama_print_timings:       total time =   20573.28 ms /   687 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      28.24 ms /   179 runs   (    0.16 ms per token,  6337.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     941.87 ms /   122 tokens (    7.72 ms per token,   129.53 tokens per second)\n",
      "llama_print_timings:        eval time =    6023.61 ms /   178 runs   (   33.84 ms per token,    29.55 tokens per second)\n",
      "llama_print_timings:       total time =    7189.19 ms /   300 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      28.31 ms /   169 runs   (    0.17 ms per token,  5970.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1304.45 ms /   282 tokens (    4.63 ms per token,   216.18 tokens per second)\n",
      "llama_print_timings:        eval time =    5699.79 ms /   168 runs   (   33.93 ms per token,    29.47 tokens per second)\n",
      "llama_print_timings:       total time =    7218.74 ms /   450 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      42.25 ms /   259 runs   (    0.16 ms per token,  6129.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     940.20 ms /   115 tokens (    8.18 ms per token,   122.31 tokens per second)\n",
      "llama_print_timings:        eval time =    8740.40 ms /   258 runs   (   33.88 ms per token,    29.52 tokens per second)\n",
      "llama_print_timings:       total time =   10014.13 ms /   373 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      32.10 ms /   198 runs   (    0.16 ms per token,  6168.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     976.40 ms /   138 tokens (    7.08 ms per token,   141.34 tokens per second)\n",
      "llama_print_timings:        eval time =    6668.77 ms /   197 runs   (   33.85 ms per token,    29.54 tokens per second)\n",
      "llama_print_timings:       total time =    7893.38 ms /   335 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      12.78 ms /    76 runs   (    0.17 ms per token,  5947.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     908.14 ms /    80 tokens (   11.35 ms per token,    88.09 tokens per second)\n",
      "llama_print_timings:        eval time =    2537.47 ms /    75 runs   (   33.83 ms per token,    29.56 tokens per second)\n",
      "llama_print_timings:       total time =    3538.74 ms /   155 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      24.61 ms /   152 runs   (    0.16 ms per token,  6176.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     934.43 ms /   109 tokens (    8.57 ms per token,   116.65 tokens per second)\n",
      "llama_print_timings:        eval time =    5103.36 ms /   151 runs   (   33.80 ms per token,    29.59 tokens per second)\n",
      "llama_print_timings:       total time =    6223.58 ms /   260 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      27.61 ms /   169 runs   (    0.16 ms per token,  6120.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     990.18 ms /   160 tokens (    6.19 ms per token,   161.59 tokens per second)\n",
      "llama_print_timings:        eval time =    5684.09 ms /   168 runs   (   33.83 ms per token,    29.56 tokens per second)\n",
      "llama_print_timings:       total time =    6882.19 ms /   328 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      73.78 ms /   455 runs   (    0.16 ms per token,  6166.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     951.59 ms /   128 tokens (    7.43 ms per token,   134.51 tokens per second)\n",
      "llama_print_timings:        eval time =   15398.50 ms /   454 runs   (   33.92 ms per token,    29.48 tokens per second)\n",
      "llama_print_timings:       total time =   16955.06 ms /   582 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      46.80 ms /   290 runs   (    0.16 ms per token,  6196.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     988.99 ms /   153 tokens (    6.46 ms per token,   154.70 tokens per second)\n",
      "llama_print_timings:        eval time =    9787.43 ms /   289 runs   (   33.87 ms per token,    29.53 tokens per second)\n",
      "llama_print_timings:       total time =   11143.23 ms /   442 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      27.27 ms /   181 runs   (    0.15 ms per token,  6636.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1045.57 ms /   176 tokens (    5.94 ms per token,   168.33 tokens per second)\n",
      "llama_print_timings:        eval time =    6094.56 ms /   180 runs   (   33.86 ms per token,    29.53 tokens per second)\n",
      "llama_print_timings:       total time =    7363.25 ms /   356 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      29.59 ms /   187 runs   (    0.16 ms per token,  6319.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     939.47 ms /   121 tokens (    7.76 ms per token,   128.80 tokens per second)\n",
      "llama_print_timings:        eval time =    6296.26 ms /   186 runs   (   33.85 ms per token,    29.54 tokens per second)\n",
      "llama_print_timings:       total time =    7468.75 ms /   307 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      14.29 ms /    87 runs   (    0.16 ms per token,  6090.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     941.74 ms /   123 tokens (    7.66 ms per token,   130.61 tokens per second)\n",
      "llama_print_timings:        eval time =    2908.12 ms /    86 runs   (   33.82 ms per token,    29.57 tokens per second)\n",
      "llama_print_timings:       total time =    3956.03 ms /   209 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      34.66 ms /   214 runs   (    0.16 ms per token,  6173.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     939.44 ms /   116 tokens (    8.10 ms per token,   123.48 tokens per second)\n",
      "llama_print_timings:        eval time =    7209.72 ms /   213 runs   (   33.85 ms per token,    29.54 tokens per second)\n",
      "llama_print_timings:       total time =    8418.74 ms /   329 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      32.46 ms /   202 runs   (    0.16 ms per token,  6222.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     936.45 ms /   111 tokens (    8.44 ms per token,   118.53 tokens per second)\n",
      "llama_print_timings:        eval time =    6801.98 ms /   201 runs   (   33.84 ms per token,    29.55 tokens per second)\n",
      "llama_print_timings:       total time =    7993.90 ms /   312 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      31.83 ms /   194 runs   (    0.16 ms per token,  6094.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     947.02 ms /   122 tokens (    7.76 ms per token,   128.83 tokens per second)\n",
      "llama_print_timings:        eval time =    6530.01 ms /   193 runs   (   33.83 ms per token,    29.56 tokens per second)\n",
      "llama_print_timings:       total time =    7720.12 ms /   315 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      20.50 ms /   128 runs   (    0.16 ms per token,  6244.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     930.88 ms /   100 tokens (    9.31 ms per token,   107.43 tokens per second)\n",
      "llama_print_timings:        eval time =    4290.87 ms /   127 runs   (   33.79 ms per token,    29.60 tokens per second)\n",
      "llama_print_timings:       total time =    5378.24 ms /   227 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      31.43 ms /   195 runs   (    0.16 ms per token,  6204.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     947.33 ms /   127 tokens (    7.46 ms per token,   134.06 tokens per second)\n",
      "llama_print_timings:        eval time =    6563.53 ms /   194 runs   (   33.83 ms per token,    29.56 tokens per second)\n",
      "llama_print_timings:       total time =    7752.59 ms /   321 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      30.10 ms /   182 runs   (    0.17 ms per token,  6045.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     973.35 ms /   129 tokens (    7.55 ms per token,   132.53 tokens per second)\n",
      "llama_print_timings:        eval time =    6125.08 ms /   181 runs   (   33.84 ms per token,    29.55 tokens per second)\n",
      "llama_print_timings:       total time =    7327.90 ms /   310 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      35.27 ms /   220 runs   (    0.16 ms per token,  6237.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     966.85 ms /   140 tokens (    6.91 ms per token,   144.80 tokens per second)\n",
      "llama_print_timings:        eval time =    7418.47 ms /   219 runs   (   33.87 ms per token,    29.52 tokens per second)\n",
      "llama_print_timings:       total time =    8664.53 ms /   359 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     385.07 ms\n",
      "llama_print_timings:      sample time =      22.51 ms /   137 runs   (    0.16 ms per token,  6085.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     980.20 ms /   147 tokens (    6.67 ms per token,   149.97 tokens per second)\n",
      "llama_print_timings:        eval time =    4596.84 ms /   136 runs   (   33.80 ms per token,    29.59 tokens per second)\n",
      "llama_print_timings:       total time =    5745.24 ms /   283 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "print(len(ds), \" records\")\n",
    "while True:\n",
    "\n",
    "    #回答\n",
    "    records = prepare_records(\n",
    "        ds, mode_list, n_records=n_records,\n",
    "        inst_dict=inst_dict\n",
    "        \n",
    "        )\n",
    "    prompts = [record[\"prompt\"] for record in records]\n",
    "\n",
    "    outputs1=[]\n",
    "    for prompt in prompts:\n",
    "        outputs1.append(bot.ask(prompt))\n",
    "    \n",
    "    for record, ja_output in zip(records, outputs1):\n",
    "        ja= (ja_output).strip()\n",
    "        #ja=clean(ja,lang=\"ja\")\n",
    "\n",
    "        if ja==\"\":\n",
    "            #print(\"rejected\")\n",
    "            #print(ja_output.outputs[0].text)\n",
    "            continue\n",
    "\n",
    "\n",
    "        record[\"ja\"]=ja \n",
    "        record.pop(\"prompt\")\n",
    "\n",
    "\n",
    "        #print(\"saving to \"+out_path)\n",
    "        with open(out_path, \"a\") as f:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am not a professional translator for Japanese, but I can help translate the English sentences into casual Japanese using translation tools and resources. However, please note that the quality of machine-generated translations may vary, and it\\'s always best to consult with a native speaker or a professional translator for accurate results.\\n\\nQ: Translate \"Don\\'t think about killing yourself again!\" to Japanese (casual)?\\n\\nA: もう一度自殺を考えてはいけない！(Mō ichido jisatsu o kangaete wa ikenai!)\\n\\nQ: Translate \"It\\'s not that we were wanting to talk to you.\" to Japanese (casual)?\\n\\nA: 俺たちが君と話していたかったわけじゃないんだよ。(Oretachi ga kimi to hanashite itakatta wake janain da yo.)\\n\\nQ: Translate \"I don\\'t recognize the bottle.\" to Japanese (casual)?\\n\\nA: あのボトルが分からない。(Ano botoru ga wakaranai.)',\n",
       " 'The provided sentences are in Finnish language and here is the translation of each sentence into Japanese:\\n\\n1) EnglishTästä asiassta olen huoissani.\\n   -> この問題について心配しています。\\n\\n2) Nämä järjestelmät on tärkeää saada toimintaaan, joatta vuoden 2008 säänteilykriisin toistuminen voidaan estää.\\n   -> これらのシステムを稼働させることは重要です。そうしないと、2008年の規制クリースが再発する可能性があります。\\n\\n3) Tällaiset rahasto saattaavat olla keksiisessä asennassa kehitettäessä energitatehokkuaan liittyviä palvelujia tuottavia PK-yrityksiä ja yhtiöitä.\\n   -> このような資本がエネルギー効率に関連するサービスを提供する企業や会社を開発する際に中心的な役割を果たす可能性があります。\\n\\n4) Liika keksittyneen Kioton sopimukseen väheiseen määrällisen vaikutuksen ja kasvihuonekaaspäästöjen vähentämisen välittömiiin tuloksiin merkitsee kuitenkin sitä, että asian ydin on jäänyt ymmärtämättä.\\n   -> このアグリーメントについて、気温が過度に注目され、緩和効果や直接的な結果についての議論が欠けていることを意味します。そのため、本質が理解できていない状態になっています。']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'original_text': 'You are a professional translator. Translate the following Englsh into fluent Japanese.\\nUse casual Japanese.\\n#EnglishQuestion:\\nImagine the conversation that came before this response?\\nBlack Veil Brides (album)  Black Veil Brides, also known as Black Veil Brides IV, is the self-titled fourth studio album by American rock band Black Veil Brides.\\n\\nAnswer:\\nDIALOG:\\nWhat is Black Veil Brides?\\n- \\n\\n\\nQuestion:\\nImagine the conversation that came before this response?\\nDanielle Lawrie Danielle Elaine Lawrie-Locke (born April 11, 1987) is a Canadian softball pitcher and current sports commentator.\\n\\nAnswer:\\nDIALOG:\\nWho is Danielle Lawrie?\\n- \\n\\n\\nQuestion:\\nImagine the conversation that came before this response?\\nHis most memorable performance was in punjab and from there he got a nick name Mohit Bansal Chandigarh.\\n\\nAnswer:\\nDIALOG:\\nWho is Mohit Thadani?\\n- Mohit Thadani Mohit Kishore Thadani (born 2 July 1991) is a Rajasthani cricketer.\\n- What happen in the Irani Cup?\\n- He plays for Gujarat and made his debut in First-class cricket on 20 January 2017 against Rest of India in 2016–17 Irani Cup.\\n- What else did you find interesting?\\n- \\n\\n\\nQuestion:\\nImagine the conversation that came before this response?\\nGillette Field Airport Gillette Field Airport is a private airport located 8 miles east of Scio in Linn County, Oregon, USA.\\n\\nAnswer:\\n\\nDIALOG:\\nWhat is Gillette Field Airport?\\n-',\n",
       "  'mode': 'translate5',\n",
       "  'en': 'Question:\\nImagine the conversation that came before this response?\\nBlack Veil Brides (album)  Black Veil Brides, also known as Black Veil Brides IV, is the self-titled fourth studio album by American rock band Black Veil Brides.\\n\\nAnswer:\\nDIALOG:\\nWhat is Black Veil Brides?\\n- \\n\\n\\nQuestion:\\nImagine the conversation that came before this response?\\nDanielle Lawrie Danielle Elaine Lawrie-Locke (born April 11, 1987) is a Canadian softball pitcher and current sports commentator.\\n\\nAnswer:\\nDIALOG:\\nWho is Danielle Lawrie?\\n- \\n\\n\\nQuestion:\\nImagine the conversation that came before this response?\\nHis most memorable performance was in punjab and from there he got a nick name Mohit Bansal Chandigarh.\\n\\nAnswer:\\nDIALOG:\\nWho is Mohit Thadani?\\n- Mohit Thadani Mohit Kishore Thadani (born 2 July 1991) is a Rajasthani cricketer.\\n- What happen in the Irani Cup?\\n- He plays for Gujarat and made his debut in First-class cricket on 20 January 2017 against Rest of India in 2016–17 Irani Cup.\\n- What else did you find interesting?\\n- \\n\\n\\nQuestion:\\nImagine the conversation that came before this response?\\nGillette Field Airport Gillette Field Airport is a private airport located 8 miles east of Scio in Linn County, Oregon, USA.\\n\\nAnswer:\\n\\nDIALOG:\\nWhat is Gillette Field Airport?\\n-'},\n",
       " {'original_text': 'You are a professional translator. Translate the following Englsh into fluent Japanese.\\n#EnglishQUESTION: Write another turn of this conversation. DIALOG:\\nWhat was the City of the Caesars?\\n- \\nANS: City of the Caesars The City of the Caesars (Spanish Ciudad de los Césares), also variously known as \"City of Patagonia\", \"the Wandering City\", \"Trapalanda\" or \"Trapananda\", \"Lin Lin\" or \"Elelín\", is a mythical city of South America.\\n\\nQUESTION: Write another turn of this conversation. DIALOG:\\nWhat was the biggest project you worked on during your engineering career?\\n- Gordon Lewis spent some years as Managing Director of Turbo-Union, which developed the Turbo-Union RB199 engine for the Panavia Tornado.\\n- Did you work on any other projects?\\n- His patented devices cover a forty-year span, and he remained active in many aspects of turbine engine work well into the 21st century.\\n- Are there any other interesting aspects about this article?\\n- \\nANS: He died in Bristol on 4 October 2010.\\n\\nQUESTION: Write another turn of this conversation. DIALOG:\\nWhat does Borealism mean in regards to the Dutch election?\\n- \\nANS: At the first party congress of FvD in 2017, he spoke about \"our boreal Europe\"; in the victory speech he gave to his followers after the 2019 Dutch provincial elections, about \"our boreal world\".\\n\\nQUESTION: Write another turn of this conversation. DIALOG:\\nWhat was the Prem Rawat\\'s Golden Years?\\n- People said, \"He\\'s going to fade away.\" Well, how about fifty-two years.\\n- Are there any other interesting aspects about this article?\\n- And I\\'m still doing strong, because it is about my conviction.\\n- What was Rawat\\'s conviction?\\n- \\nANS:\\nAnd my conviction is \"peace is possible\".',\n",
       "  'mode': 'translate1',\n",
       "  'en': 'QUESTION: Write another turn of this conversation. DIALOG:\\nWhat was the City of the Caesars?\\n- \\nANS: City of the Caesars The City of the Caesars (Spanish Ciudad de los Césares), also variously known as \"City of Patagonia\", \"the Wandering City\", \"Trapalanda\" or \"Trapananda\", \"Lin Lin\" or \"Elelín\", is a mythical city of South America.\\n\\nQUESTION: Write another turn of this conversation. DIALOG:\\nWhat was the biggest project you worked on during your engineering career?\\n- Gordon Lewis spent some years as Managing Director of Turbo-Union, which developed the Turbo-Union RB199 engine for the Panavia Tornado.\\n- Did you work on any other projects?\\n- His patented devices cover a forty-year span, and he remained active in many aspects of turbine engine work well into the 21st century.\\n- Are there any other interesting aspects about this article?\\n- \\nANS: He died in Bristol on 4 October 2010.\\n\\nQUESTION: Write another turn of this conversation. DIALOG:\\nWhat does Borealism mean in regards to the Dutch election?\\n- \\nANS: At the first party congress of FvD in 2017, he spoke about \"our boreal Europe\"; in the victory speech he gave to his followers after the 2019 Dutch provincial elections, about \"our boreal world\".\\n\\nQUESTION: Write another turn of this conversation. DIALOG:\\nWhat was the Prem Rawat\\'s Golden Years?\\n- People said, \"He\\'s going to fade away.\" Well, how about fifty-two years.\\n- Are there any other interesting aspects about this article?\\n- And I\\'m still doing strong, because it is about my conviction.\\n- What was Rawat\\'s conviction?\\n- \\nANS:\\nAnd my conviction is \"peace is possible\".'},\n",
       " {'original_text': 'You are a translator. Translate the following Englsh into fluent Japanese.\\n#EnglishDIALOG:\\nWhen was the 1973 Haitian parliamentary election?\\n- 1973 Haitian parliamentary election Parliamentary elections were held in Haiti on 11 February 1973.\\n- Who was elected?\\n- \\nNext turn: Over 300 candidates contested the election, all of whom were members of the National Unity Party and supporters of President Jean-Claude Duvalier.\\nProblem: What could be the response? DIALOG:\\nWhat is Local H\\'s Awesome Mix Tape 2?\\n- \\nA: Local H\\'s Awesome Mix Tape 2 Local H\\'s Awesome Mix Tape #2 is an extended play by American alternative rock duo Local H, which was released in December 2014 through their merchandiser, G&P Records.\\nQUESTION: Write another turn of this conversation. DIALOG:\\nWhat are some examples of the visual arts?\\n- \\nANS: From October 2009 through February 2010, the Museum of the History of Science, Oxford, hosted the first major exhibition of steampunk art objects, curated and developed by New York artist and designer Art Donovan, who also exhibited his own \"electro-futuristic\" lighting sculptures, and presented by Dr. Jim Bennett, museum director.\\nquestion: Write a response. DIALOG:\\nWhat is the incurable autoimmune response for The 100?\\n- Afterwards, Clarke rescues her and request she fly them to The Ark in which she agrees to do, and successfully able to fly them to space and finally was able to spacewalk again.\\n- Are there any other interesting aspects about this article?\\n- In the fifth season, Raven becomes an improved fighter being able to topple Echo in combat and plans to return to earth she soon notices another ship landing on Earth, and proceeds to follow it, she realizes there is other life forms in cryostasis on the Eligius ship and decides to stay behind and lies about an escape pod so Bellamy will not argue with her.\\n- Who is Lincoln?\\n- Portrayed by Ricky Whittle, Lincoln (seasons 2–3; recurring season 1) is a Grounder who rescues Octavia.\\n- What happened with Octavia?\\n- The pair develop a romantic relationship, and Lincoln helps the 100 multiple times, causing him to be viewed as a traitor to his people.\\n- What happened after that?\\n- \\nresponse: He later is drugged to become a reaper, but with Clarke\\'s and Abby\\'s assistance, he is successfully rehabilitated from the drug.\\nQ: See the conversation. DIALOG:\\nWhat awards did James Harvey win?\\n- He was subsequently named to the All-NBL second team in 2007–08 and 2008–09.\\n- What happened during James Harvey\\'s professional career?\\n- On 15 May 2012, Harvey signed a two-year deal with the Sydney Kings.\\n- Was Harvey a leader for the team?\\n- During his time with the Kings, he was the team\\'s co-captain.\\n- Are there any other interesting aspects about this article?\\n- \\n****\\nNext: On 4 February 2013, Harvey was named in the Perth Wildcats 30th Anniversary All-Star team.\\nSee the conversation examples, and predict the next turn. DIALOG:\\nWhat was the impact of the Portuguese colonial education on Mozambique?\\n- A school for Mozambican exiles was founded in Dar es Salaam in the 1960s, though by 1967 it still had only 150 students.\\n- Are there any other interesting aspects about this article?\\n- The school foundered following the assassination of Eduardo Mondlane in 1969.\\n- What other programs did the school system do?\\n- \\n\\n\\nAs the organisation took control of areas of Mozambique in the 1970s, it promoted education among both children and adults.',\n",
       "  'mode': 'translate2',\n",
       "  'en': 'DIALOG:\\nWhen was the 1973 Haitian parliamentary election?\\n- 1973 Haitian parliamentary election Parliamentary elections were held in Haiti on 11 February 1973.\\n- Who was elected?\\n- \\nNext turn: Over 300 candidates contested the election, all of whom were members of the National Unity Party and supporters of President Jean-Claude Duvalier.\\nProblem: What could be the response? DIALOG:\\nWhat is Local H\\'s Awesome Mix Tape 2?\\n- \\nA: Local H\\'s Awesome Mix Tape 2 Local H\\'s Awesome Mix Tape #2 is an extended play by American alternative rock duo Local H, which was released in December 2014 through their merchandiser, G&P Records.\\nQUESTION: Write another turn of this conversation. DIALOG:\\nWhat are some examples of the visual arts?\\n- \\nANS: From October 2009 through February 2010, the Museum of the History of Science, Oxford, hosted the first major exhibition of steampunk art objects, curated and developed by New York artist and designer Art Donovan, who also exhibited his own \"electro-futuristic\" lighting sculptures, and presented by Dr. Jim Bennett, museum director.\\nquestion: Write a response. DIALOG:\\nWhat is the incurable autoimmune response for The 100?\\n- Afterwards, Clarke rescues her and request she fly them to The Ark in which she agrees to do, and successfully able to fly them to space and finally was able to spacewalk again.\\n- Are there any other interesting aspects about this article?\\n- In the fifth season, Raven becomes an improved fighter being able to topple Echo in combat and plans to return to earth she soon notices another ship landing on Earth, and proceeds to follow it, she realizes there is other life forms in cryostasis on the Eligius ship and decides to stay behind and lies about an escape pod so Bellamy will not argue with her.\\n- Who is Lincoln?\\n- Portrayed by Ricky Whittle, Lincoln (seasons 2–3; recurring season 1) is a Grounder who rescues Octavia.\\n- What happened with Octavia?\\n- The pair develop a romantic relationship, and Lincoln helps the 100 multiple times, causing him to be viewed as a traitor to his people.\\n- What happened after that?\\n- \\nresponse: He later is drugged to become a reaper, but with Clarke\\'s and Abby\\'s assistance, he is successfully rehabilitated from the drug.\\nQ: See the conversation. DIALOG:\\nWhat awards did James Harvey win?\\n- He was subsequently named to the All-NBL second team in 2007–08 and 2008–09.\\n- What happened during James Harvey\\'s professional career?\\n- On 15 May 2012, Harvey signed a two-year deal with the Sydney Kings.\\n- Was Harvey a leader for the team?\\n- During his time with the Kings, he was the team\\'s co-captain.\\n- Are there any other interesting aspects about this article?\\n- \\n****\\nNext: On 4 February 2013, Harvey was named in the Perth Wildcats 30th Anniversary All-Star team.\\nSee the conversation examples, and predict the next turn. DIALOG:\\nWhat was the impact of the Portuguese colonial education on Mozambique?\\n- A school for Mozambican exiles was founded in Dar es Salaam in the 1960s, though by 1967 it still had only 150 students.\\n- Are there any other interesting aspects about this article?\\n- The school foundered following the assassination of Eduardo Mondlane in 1969.\\n- What other programs did the school system do?\\n- \\n\\n\\nAs the organisation took control of areas of Mozambique in the 1970s, it promoted education among both children and adults.'},\n",
       " {'original_text': \"You are a professional translator. Translate the following Englsh into fluent Japanese.\\nUse casual Japanese.\\n#EnglishConsider this response: Stroh was a sponsor of the 1982 World's Fair in Knoxville, Tennessee, an event that strengthened Stroh's new national standing considerably.\\nWhat was the preceding dialog?\\nPreceding dialog: DIALOG:\\nWhat was the most successful product from the Stroh Brewery Company's history?\\n- Stroh TV commercials included:   Locally in Michigan, the company was the longtime sponsor for the Detroit Tigers baseball team's radio broadcasts, a relationship that began in 1960.\\n- When did the company start expanding?\\n- In the 1980s, the company also turned to corporate sponsorship to gain needed national publicity.\\n- What year was the company formed?\\n- \\nResponse: Viitorul Constanța The preceding conversation:\\n\\nPreceding conversation: DIALOG:\\nWhat was Gheorghe Hagi's first professional contract?\\n- \\nWrite an example conversation that led to this. This: 34-year-old centre half Joe James elected to retire six months after failing to recover from a wrist injury suffered in the match versus Charlton Athletic on 26 February 1944.\\nPreceding conversation: DIALOG:\\nWhat were some of the highlights of the 1943–44 Brentford F.C. season?\\n- \\nSee the last examples. Predict the preceding dialog. DIALOG:\\nWhat is the capital city of Barbara Frangella?\\n- \\nPreceding conversation: DIALOG:\\nWhat is the capital city of Barbara Frangella?\\n- \\nProblem: If this is the response, what came before? Undated: Other entries have included JJ72 and Bass Odyssey.\\n++++++++++\\nBefore this should be: DIALOG:\\nWhat is the Eurosonic Festival?\\n- Ireland in the Eurosonic Festival Each year Ireland selects a number of bands and musicians to participate in the Eurosonic Festival in Groningen, Netherlands.\\n- Are there any other interesting aspects about this article?\\n- In recent years, RTÉ 2fm, have sponsored two bands per festival.\\n- What bands have they sponsored?\\n- \\nInput: What came before. Jeff Gordon won the pole.\\nSolution:\\nDIALOG:\\nWhat race happened at the Budweiser 200?\\n- The Budweiser 200 was held June 1 at Dover International Speedway.\\n- Who won the race?\\n- Dave Madar III won the pole.\\n- What other races did they have?\\n- Top ten results The Roses Stores 300 was held June 8 at Orange County Speedway.\\n- Who won that race?\\n- \",\n",
       "  'mode': 'translate5',\n",
       "  'en': \"Consider this response: Stroh was a sponsor of the 1982 World's Fair in Knoxville, Tennessee, an event that strengthened Stroh's new national standing considerably.\\nWhat was the preceding dialog?\\nPreceding dialog: DIALOG:\\nWhat was the most successful product from the Stroh Brewery Company's history?\\n- Stroh TV commercials included:   Locally in Michigan, the company was the longtime sponsor for the Detroit Tigers baseball team's radio broadcasts, a relationship that began in 1960.\\n- When did the company start expanding?\\n- In the 1980s, the company also turned to corporate sponsorship to gain needed national publicity.\\n- What year was the company formed?\\n- \\nResponse: Viitorul Constanța The preceding conversation:\\n\\nPreceding conversation: DIALOG:\\nWhat was Gheorghe Hagi's first professional contract?\\n- \\nWrite an example conversation that led to this. This: 34-year-old centre half Joe James elected to retire six months after failing to recover from a wrist injury suffered in the match versus Charlton Athletic on 26 February 1944.\\nPreceding conversation: DIALOG:\\nWhat were some of the highlights of the 1943–44 Brentford F.C. season?\\n- \\nSee the last examples. Predict the preceding dialog. DIALOG:\\nWhat is the capital city of Barbara Frangella?\\n- \\nPreceding conversation: DIALOG:\\nWhat is the capital city of Barbara Frangella?\\n- \\nProblem: If this is the response, what came before? Undated: Other entries have included JJ72 and Bass Odyssey.\\n++++++++++\\nBefore this should be: DIALOG:\\nWhat is the Eurosonic Festival?\\n- Ireland in the Eurosonic Festival Each year Ireland selects a number of bands and musicians to participate in the Eurosonic Festival in Groningen, Netherlands.\\n- Are there any other interesting aspects about this article?\\n- In recent years, RTÉ 2fm, have sponsored two bands per festival.\\n- What bands have they sponsored?\\n- \\nInput: What came before. Jeff Gordon won the pole.\\nSolution:\\nDIALOG:\\nWhat race happened at the Budweiser 200?\\n- The Budweiser 200 was held June 1 at Dover International Speedway.\\n- Who won the race?\\n- Dave Madar III won the pole.\\n- What other races did they have?\\n- Top ten results The Roses Stores 300 was held June 8 at Orange County Speedway.\\n- Who won that race?\\n- \"},\n",
       " {'original_text': 'You are a translator. Translate the following Englsh into fluent Japanese.\\n#EnglishQuestion:\\nImagine the conversation that came before this response?\\nWhile Tsedenbal was visiting Moscow in August 1984, his severe illness prompted the parliament to announce his retirement and replace him with Jambyn Batmönkh.\\n\\nAnswer:\\nDIALOG:\\nWhen did the Parliament of Mongolia pass the Fundamental Principles of the Constitution of Mongolia?\\n- \\n\\n\\nQuestion:\\nImagine the conversation that came before this response?\\nReelected in 2001, 2008, 2014.\\n\\nAnswer:\\nDIALOG:\\nWhat did Francois Baroin do in 1995?\\n- Municipal councillor of Troyes : Since 1995.\\n- What happened in 2001?\\n- \\n\\n\\nQuestion:\\nImagine the conversation that came before this response?\\n2012 Northern Colorado Bears football team The 2012 Northern Colorado Bears football team represented the University of Northern Colorado in the 2012 NCAA Division I FCS football season.\\n\\nAnswer:\\nDIALOG:\\nWhat was the 2012 Northern Colorado Bears football team?\\n- \\n\\n\\nQuestion:\\nImagine the conversation that came before this response?\\n1999–2000 Tottenham Hotspur F.C. season During the 1999–2000 season, Tottenham Hotspur participated in the FA Premier League.\\n\\nAnswer:\\n\\nDIALOG:\\nWhat was the highlight of the 1999-2000 Tottenham Hotspur F.C. season?\\n-',\n",
       "  'mode': 'translate2',\n",
       "  'en': 'Question:\\nImagine the conversation that came before this response?\\nWhile Tsedenbal was visiting Moscow in August 1984, his severe illness prompted the parliament to announce his retirement and replace him with Jambyn Batmönkh.\\n\\nAnswer:\\nDIALOG:\\nWhen did the Parliament of Mongolia pass the Fundamental Principles of the Constitution of Mongolia?\\n- \\n\\n\\nQuestion:\\nImagine the conversation that came before this response?\\nReelected in 2001, 2008, 2014.\\n\\nAnswer:\\nDIALOG:\\nWhat did Francois Baroin do in 1995?\\n- Municipal councillor of Troyes : Since 1995.\\n- What happened in 2001?\\n- \\n\\n\\nQuestion:\\nImagine the conversation that came before this response?\\n2012 Northern Colorado Bears football team The 2012 Northern Colorado Bears football team represented the University of Northern Colorado in the 2012 NCAA Division I FCS football season.\\n\\nAnswer:\\nDIALOG:\\nWhat was the 2012 Northern Colorado Bears football team?\\n- \\n\\n\\nQuestion:\\nImagine the conversation that came before this response?\\n1999–2000 Tottenham Hotspur F.C. season During the 1999–2000 season, Tottenham Hotspur participated in the FA Premier League.\\n\\nAnswer:\\n\\nDIALOG:\\nWhat was the highlight of the 1999-2000 Tottenham Hotspur F.C. season?\\n-'},\n",
       " {'original_text': 'You are a translator. Translate the following Englsh into fluent Japanese.\\n#EnglishConsider this response: Moses organized an important school at Cordova, which was independent of the gaonate and was attended by many pupils; and through him Cordova became the seat of Jewish scholarship.\\nWhat was the preceding dialog?\\nPreceding dialog: DIALOG:\\nWhat happened during the capture of Moses ben Hanoch?\\n- Hasdai ibn Shaprut, rejoicing at this event, induced the Umayyad Caliph Abd al-Rahman III to order Ibn Rumahis to forgo the higher ransom which he, in consequence, was demanding for Moses.\\n- Are there any other interesting aspects about this article?\\n- \\n\\nConsider this response: A label bearing VICTORIA is sewn on the back pockets of the trousers.\\nWhat was the preceding dialog?\\nPreceding dialog: DIALOG:\\nWhat is the difference between Victoria School and other private schools?\\n- In 2008, Victoria School became the first school in Singapore to offer Physical Education as a GCE \\'O\\' Level subject.\\n- Is there anything else that is interesting about this article?\\n- It also became one of the first four secondary schools to offer the Regional Studies Programme.\\n- What other programs does the school offer?\\n- Victoria School started offering the Integrated Programme together with Cedar Girls\\' Secondary School and Victoria Junior College in 2012.\\n- Do the students wear a uniform?\\n- The Victoria School uniform is worn at all times, apart from the maroon blazer which is worn only during formal events.\\n- What does the uniform consist of?\\n- It consists of a white short-sleeved shirt, with either khaki short trousers (for secondary 1 and 2 students) or white long trousers (for secondary 3 and 4 students).\\n- Is there anything else that is unique about the uniform?\\n- \\n\\nConsider this response: As of the census of 2010, there were 33 people, 21 households, and 10 families residing in the village.\\nWhat was the preceding dialog?\\nPreceding dialog: DIALOG:\\nWhat is the median income in Seneca, Nebraska?\\n- As of 2000, the median income for a household in the village was $20,833 and the median income for a family was $21,667. Males had a median income of $26,250 versus $20,500 for females.\\n- What is the per capita income?\\n- The per capita income for the village was $15,803.\\n- Are there any other interesting aspects about this article?\\n- The village population included 12.1% who lived below the poverty line.\\n- Who did this include?\\n- That included 16.7% of the families and 23.1% of residents older than 64, but no one younger than 18.\\n- How many people live in Seneca?\\n- \\n\\nConsider this response: Leave a Light On (Tom Walker song) \"Leave a Light On\" is a song by Scottish singer-songwriter Tom Walker.\\nWhat was the preceding dialog?\\nPreceding dialog:\\nDIALOG:\\nWhat is the song Leave a Light On about?\\n-',\n",
       "  'mode': 'translate2',\n",
       "  'en': 'Consider this response: Moses organized an important school at Cordova, which was independent of the gaonate and was attended by many pupils; and through him Cordova became the seat of Jewish scholarship.\\nWhat was the preceding dialog?\\nPreceding dialog: DIALOG:\\nWhat happened during the capture of Moses ben Hanoch?\\n- Hasdai ibn Shaprut, rejoicing at this event, induced the Umayyad Caliph Abd al-Rahman III to order Ibn Rumahis to forgo the higher ransom which he, in consequence, was demanding for Moses.\\n- Are there any other interesting aspects about this article?\\n- \\n\\nConsider this response: A label bearing VICTORIA is sewn on the back pockets of the trousers.\\nWhat was the preceding dialog?\\nPreceding dialog: DIALOG:\\nWhat is the difference between Victoria School and other private schools?\\n- In 2008, Victoria School became the first school in Singapore to offer Physical Education as a GCE \\'O\\' Level subject.\\n- Is there anything else that is interesting about this article?\\n- It also became one of the first four secondary schools to offer the Regional Studies Programme.\\n- What other programs does the school offer?\\n- Victoria School started offering the Integrated Programme together with Cedar Girls\\' Secondary School and Victoria Junior College in 2012.\\n- Do the students wear a uniform?\\n- The Victoria School uniform is worn at all times, apart from the maroon blazer which is worn only during formal events.\\n- What does the uniform consist of?\\n- It consists of a white short-sleeved shirt, with either khaki short trousers (for secondary 1 and 2 students) or white long trousers (for secondary 3 and 4 students).\\n- Is there anything else that is unique about the uniform?\\n- \\n\\nConsider this response: As of the census of 2010, there were 33 people, 21 households, and 10 families residing in the village.\\nWhat was the preceding dialog?\\nPreceding dialog: DIALOG:\\nWhat is the median income in Seneca, Nebraska?\\n- As of 2000, the median income for a household in the village was $20,833 and the median income for a family was $21,667. Males had a median income of $26,250 versus $20,500 for females.\\n- What is the per capita income?\\n- The per capita income for the village was $15,803.\\n- Are there any other interesting aspects about this article?\\n- The village population included 12.1% who lived below the poverty line.\\n- Who did this include?\\n- That included 16.7% of the families and 23.1% of residents older than 64, but no one younger than 18.\\n- How many people live in Seneca?\\n- \\n\\nConsider this response: Leave a Light On (Tom Walker song) \"Leave a Light On\" is a song by Scottish singer-songwriter Tom Walker.\\nWhat was the preceding dialog?\\nPreceding dialog:\\nDIALOG:\\nWhat is the song Leave a Light On about?\\n-'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': 'DIALOG:\\nWhat is a surname?\\n- Meintjes Meintjes is a surname.\\n- Who are some notable people with the surname?\\n- \\nNext turn: Notable people with the surname include:\\nProblem: What could be the response? DIALOG:\\nWho was Eliezer Kashani?\\n- Eliezer Kashani Eliezer Kashani (; 1923–1947) was an Irgun member in Mandatory Palestine and one of the 12 Olei Hagardom.\\n- What did he do in his early life?\\n- Kashani was born in Petah Tikva to an impoverished Persian-Jewish family with 4 brothers and 3 sisters.\\n- Did he go to school?\\n- \\nA: In his youth he was a part of the Maccabi movement, and he worked in a factory at age 13.\\nQUESTION: Write another turn of this conversation. DIALOG:\\nWhat do you know about Psila angustata?\\n- \\nANS: Psila angustata Psila angustata is a species of rust flies in the family Psilidae.\\nquestion: Write a response. DIALOG:\\nWhat is the proposed privatization of Metro State Prison?\\n- In January 2011 Owens said that a private prison company may have expressed an interest in buying the prison.\\n- Are there any other interesting aspects about this article?\\n- In April 2011 a spokesperson for the Georgia DOC said that the state had no plans for the facility.\\n- What was the reason it was closed?\\n- \\nresponse: At the time of the closure the prison had 319 employees.\\nQ: See the conversation. DIALOG:\\nWhat is Tune In, Turn On, Free Tibet?\\n- \\n****\\nNext: Tune In, Turn On, Free Tibet Tune In, Turn On, Free Tibet is an album by the band Ghost.\\nQUESTION: Write another turn of this conversation. DIALOG:\\nWhat awards did Gerald Koocher receive?\\n- \\nANS:',\n",
       " 'targets': \"United States Public Health Service Fellow, University of Missouri  1971 Psi Chi (Honorary in Psychology)  1977 Elected a Fellow of the American Psychological Association (12 Divisions)  1981 Jack D. Krasner Memorial Award, Division of Psychotherapy, American Psychological Association 1983 Distinguished Professional Contribution Award, Section on Clinical Child Psychology, American Psychological Association 1986 Distinguished Career Contribution Award, Massachusetts Psychological Association 1986 Michael Dinoff Memorial Lecturer, University of Alabama, Tuscaloosa, AL  1987 Elected to the Collegium of Distinguished Alumni, College of Liberal Arts, Boston University 1987 Distinguished Professional Contribution Award, Society of Pediatric Psychology  1987 Matthew Ryan Young Memorial Lecturer, University of Massachusetts Medical School, Worcester, MA  1988 Nicholas Hobbs Award for Distinguished Contributions to Children's Services, American Psychological Association, Division of Child, Youth and Family Services 1989 Elected a Fellow of the American association for the Advancement of Science  1988-1989 Linda Pollin Memorial Lecturer, Linda Pollin Foundation, Bethesda, MA  1992 American Psychological Association Award for Distinguished Professional Contributions to Public Service 1993 Elected to the National Academies of Practice; Distinguished Practitioner in Psychology  1993 Karl F. Heiser Presidential Award, American Psychological Association 1996 Distinguished Alumnus Award, University of Missouri, College of Arts and Sciences 1996 Robert Chin Memorial Award of the Society for the Psychological Study of Social Issues 2003 Florence Halpern Award for Distinguished Professional Contributions, Society of Clinical Psychology, American Psychological Association 2005 Distinguished Psychologist Award, Division of Psychotherapy, American Psychological Association 2005 Distinguished Career Contribution Award, Connecticut Psychological Association.\",\n",
       " '_template_idx': 7,\n",
       " '_task_source': 'Dialog',\n",
       " '_task_name': 'wiki_dialog',\n",
       " '_template_type': 'fs_opt'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
